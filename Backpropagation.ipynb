{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffde43c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64e10a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "# X = (hours sleeping, hours studying), y = test score of the student\n",
    "X = np.array([[2, 9], [1, 5], [3, 6]])\n",
    "y = np.array([[92], [86], [89]])\n",
    "\n",
    "# scale units\n",
    "X = X/np.amax(X, axis=0) #maximum of X array\n",
    "y = y/100 # maximum test score is 100\n",
    "np.random.seed(2)\n",
    "class NeuralNetwork():\n",
    "    def __init__(self,inp_size,output_size,hidden_size):\n",
    "        self.inp_size=inp_size\n",
    "        self.output_size=output_size\n",
    "        self.hidden_size=hidden_size\n",
    "        self.bias1=0\n",
    "        self.bias2=0\n",
    "        \n",
    "        self.W1=np.random.uniform(-0.5,0.5,(self.inp_size,self.hidden_size))\n",
    "        self.W2=np.random.uniform(-0.5,0.5,(self.hidden_size,self.output_size))\n",
    "    \n",
    "    def feedforward(self,X):\n",
    "        \n",
    "        self.input_sum=np.dot(X,self.W1)+self.bias1\n",
    "        self.hid_inp=self.sigmoid(self.input_sum)\n",
    "        \n",
    "        self.hid_sum=np.dot(self.hid_inp,self.W2)+self.bias2\n",
    "        output=self.sigmoid(self.hid_sum)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def backpropagation(self, X, y, output, lr):\n",
    "        self.error = y - output\n",
    "        self.output_delta = self.error * self.sigmoid_derivative(output)\n",
    "        \n",
    "        self.hidden_error = self.output_delta.dot(self.W2.T) \n",
    "        self.hidden_delta = self.hidden_error * self.sigmoid_derivative(self.hid_inp)\n",
    "\n",
    "        self.W1 += X.T.dot(self.hidden_delta) * lr\n",
    "        self.W2 += self.hid_inp.T.dot(self.output_delta) * lr\n",
    "\n",
    "        self.bias1 += np.sum(self.hidden_delta) * lr\n",
    "        self.bias2 += np.sum(self.output_delta) * lr\n",
    "\n",
    "        \n",
    "    def sigmoid(self,x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self,s):\n",
    "        return s*(1-s)\n",
    "    def train(self,X,y,epochs):\n",
    "        for i in range(epochs):\n",
    "            output=self.feedforward(X)\n",
    "            self.backpropagation(X,y,output,0.1)\n",
    "            if i%1000 == 0:\n",
    "                print(f\"Epochs {i} Loss {np.mean(np.abs(y - output))}\")\n",
    "        \n",
    "        \n",
    "# NN=NeuralNetwork()\n",
    "\n",
    "# for i in range(1000):\n",
    "#     if i%500==0:\n",
    "#         print('Loss=',np.mean(np.square(y-NN.feedforward(X))))\n",
    "#     NN.train(X,y)\n",
    "    \n",
    "# print('Result=\\n',NN.feedforward(X))\n",
    "# print('Actual:\\n',y)\n",
    "# print('Loss=',np.mean(np.square(y-NN.feedforward(X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c69547ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('Iris.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7ffc9c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
       "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
       "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
       "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
       "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
       "4   5            5.0           3.6            1.4           0.2  Iris-setosa"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "903b1258",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Species']=df['Species'].replace({'Iris-setosa':0, 'Iris-versicolor':1, 'Iris-virginica':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1fd2702",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1=np.array([df['SepalLengthCm'],df['SepalWidthCm'],df['PetalLengthCm'],df['PetalWidthCm']]).T\n",
    "arr2=np.array([df['Species']]).T\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "oe=OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b0bcb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain,xtest,ytrain,ytest=train_test_split(arr1,arr2,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10e2c8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain=oe.fit_transform(ytrain.reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50a0c674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((120, 4), (120, 3))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.shape,ytrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dd8bcf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "inpsize=xtrain.shape[1]\n",
    "outputsize=ytrain.shape[1]\n",
    "hiddensize=50\n",
    "lr=0.1\n",
    "epochs=100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5eefad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet=NeuralNetwork(inpsize,outputsize,hiddensize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f6a2c3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 0 Loss 0.5769758392656431\n",
      "Epochs 1000 Loss 0.2356152330308069\n",
      "Epochs 2000 Loss 0.029536199957720655\n",
      "Epochs 3000 Loss 0.02205081390771129\n",
      "Epochs 4000 Loss 0.020598757272238657\n",
      "Epochs 5000 Loss 0.018579193049283307\n",
      "Epochs 6000 Loss 0.019737139788934214\n",
      "Epochs 7000 Loss 0.010772593189977652\n",
      "Epochs 8000 Loss 0.009471694654085705\n",
      "Epochs 9000 Loss 0.008879454144437828\n",
      "Epochs 10000 Loss 0.008512889957763586\n",
      "Epochs 11000 Loss 0.008276721420891336\n",
      "Epochs 12000 Loss 0.008148110298020591\n",
      "Epochs 13000 Loss 0.008176962600226656\n",
      "Epochs 14000 Loss 0.008745664522693295\n",
      "Epochs 15000 Loss 0.010743925199450506\n",
      "Epochs 16000 Loss 0.012277240714185258\n",
      "Epochs 17000 Loss 0.012742893972579158\n",
      "Epochs 18000 Loss 0.01285330870957071\n",
      "Epochs 19000 Loss 0.012809160394166094\n",
      "Epochs 20000 Loss 0.01278420577604597\n",
      "Epochs 21000 Loss 0.012824631545158935\n",
      "Epochs 22000 Loss 0.011670689558864253\n",
      "Epochs 23000 Loss 0.01093192560265399\n",
      "Epochs 24000 Loss 0.013138552097142897\n",
      "Epochs 25000 Loss 0.016332644634439324\n",
      "Epochs 26000 Loss 0.01296479800651459\n",
      "Epochs 27000 Loss 0.01641561177459422\n",
      "Epochs 28000 Loss 0.01105511158404463\n",
      "Epochs 29000 Loss 0.011372028966911824\n",
      "Epochs 30000 Loss 0.012419184665543842\n",
      "Epochs 31000 Loss 0.01288710166579432\n",
      "Epochs 32000 Loss 0.011268810480539621\n",
      "Epochs 33000 Loss 0.011609915850050652\n",
      "Epochs 34000 Loss 0.011761433901518\n",
      "Epochs 35000 Loss 0.009629639919503644\n",
      "Epochs 36000 Loss 0.013449862843391782\n",
      "Epochs 37000 Loss 0.008107598135054826\n",
      "Epochs 38000 Loss 0.013203632692955715\n",
      "Epochs 39000 Loss 0.012004943812460181\n",
      "Epochs 40000 Loss 0.010830670961016869\n",
      "Epochs 41000 Loss 0.009523446825215597\n",
      "Epochs 42000 Loss 0.0051270311197381905\n",
      "Epochs 43000 Loss 0.0042223077751346845\n",
      "Epochs 44000 Loss 0.003703327541338946\n",
      "Epochs 45000 Loss 0.0033561522376010077\n",
      "Epochs 46000 Loss 0.003095850116514317\n",
      "Epochs 47000 Loss 0.0028881838950278785\n",
      "Epochs 48000 Loss 0.002716153257273552\n",
      "Epochs 49000 Loss 0.002570072923293645\n",
      "Epochs 50000 Loss 0.0024438297858959063\n",
      "Epochs 51000 Loss 0.0023332653061410246\n",
      "Epochs 52000 Loss 0.0022353944023832777\n",
      "Epochs 53000 Loss 0.002147989734632526\n",
      "Epochs 54000 Loss 0.002069341119074126\n",
      "Epochs 55000 Loss 0.001998106334212197\n",
      "Epochs 56000 Loss 0.0019332133880665642\n",
      "Epochs 57000 Loss 0.0018737937014540761\n",
      "Epochs 58000 Loss 0.0018191348761054475\n",
      "Epochs 59000 Loss 0.0017686464091640544\n",
      "Epochs 60000 Loss 0.001721834263632911\n",
      "Epochs 61000 Loss 0.0016782816678489595\n",
      "Epochs 62000 Loss 0.0016376343986671071\n",
      "Epochs 63000 Loss 0.0015995893555699819\n",
      "Epochs 64000 Loss 0.001563885590878877\n",
      "Epochs 65000 Loss 0.0015302971996897885\n",
      "Epochs 66000 Loss 0.0014986276353415077\n",
      "Epochs 67000 Loss 0.0014687051268236252\n",
      "Epochs 68000 Loss 0.0014403789461963576\n",
      "Epochs 69000 Loss 0.0014135163118284995\n",
      "Epochs 70000 Loss 0.0013879997213768294\n",
      "Epochs 71000 Loss 0.0013637245043751464\n",
      "Epochs 72000 Loss 0.001340596415111686\n",
      "Epochs 73000 Loss 0.0013185292210629671\n",
      "Epochs 74000 Loss 0.001297442500300511\n",
      "Epochs 75000 Loss 0.0012772601085586277\n",
      "Epochs 76000 Loss 0.0012579097525315889\n",
      "Epochs 77000 Loss 0.0012393236974932137\n",
      "Epochs 78000 Loss 0.0012214401138763826\n",
      "Epochs 79000 Loss 0.0012042043608721704\n",
      "Epochs 80000 Loss 0.0011875697404964143\n",
      "Epochs 81000 Loss 0.001171497659323683\n",
      "Epochs 82000 Loss 0.0011559573778197134\n",
      "Epochs 83000 Loss 0.0011409255053070773\n",
      "Epochs 84000 Loss 0.0011263852198992984\n",
      "Epochs 85000 Loss 0.0011123250387954143\n",
      "Epochs 86000 Loss 0.001098736988417515\n",
      "Epochs 87000 Loss 0.0010856142730412872\n",
      "Epochs 88000 Loss 0.0010729488875972433\n",
      "Epochs 89000 Loss 0.0010607298042083478\n",
      "Epochs 90000 Loss 0.0010489421981594485\n",
      "Epochs 91000 Loss 0.0010375677561517069\n",
      "Epochs 92000 Loss 0.001026585718660655\n",
      "Epochs 93000 Loss 0.0010159741682071672\n",
      "Epochs 94000 Loss 0.0010057111791816867\n",
      "Epochs 95000 Loss 0.0009957756450354204\n",
      "Epochs 96000 Loss 0.0009861477650450077\n",
      "Epochs 97000 Loss 0.000976809259422097\n",
      "Epochs 98000 Loss 0.0009677434027359657\n",
      "Epochs 99000 Loss 0.0009589349527979079\n"
     ]
    }
   ],
   "source": [
    "nnet.train(xtrain,ytrain,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e3de35c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SL|SW|PL|PW \t\t Predicted Class \t\t Actual Class\n",
      "[4.6 3.4 1.4 0.3]\t\t0\t\t[0]\n",
      "[4.6 3.1 1.5 0.2]\t\t0\t\t[0]\n",
      "[5.7 2.5 5.  2. ]\t\t2\t\t[2]\n",
      "[4.8 3.  1.4 0.1]\t\t0\t\t[0]\n",
      "[4.8 3.4 1.9 0.2]\t\t0\t\t[0]\n",
      "[7.2 3.  5.8 1.6]\t\t2\t\t[2]\n",
      "[5.  3.  1.6 0.2]\t\t0\t\t[0]\n",
      "[6.7 2.5 5.8 1.8]\t\t2\t\t[2]\n",
      "[6.4 2.8 5.6 2.1]\t\t2\t\t[2]\n",
      "[4.8 3.  1.4 0.3]\t\t0\t\t[0]\n",
      "[5.3 3.7 1.5 0.2]\t\t0\t\t[0]\n",
      "[4.4 3.2 1.3 0.2]\t\t0\t\t[0]\n",
      "[5.  3.2 1.2 0.2]\t\t0\t\t[0]\n",
      "[5.4 3.9 1.7 0.4]\t\t0\t\t[0]\n",
      "[6.  3.4 4.5 1.6]\t\t1\t\t[1]\n",
      "[6.5 2.8 4.6 1.5]\t\t1\t\t[1]\n",
      "[4.5 2.3 1.3 0.3]\t\t0\t\t[0]\n",
      "[5.7 2.9 4.2 1.3]\t\t1\t\t[1]\n",
      "[6.7 3.3 5.7 2.5]\t\t2\t\t[2]\n",
      "[5.5 2.5 4.  1.3]\t\t1\t\t[1]\n",
      "[6.7 3.  5.  1.7]\t\t2\t\t[1]\n",
      "[6.4 2.9 4.3 1.3]\t\t1\t\t[1]\n",
      "[6.4 3.2 5.3 2.3]\t\t2\t\t[2]\n",
      "[5.6 2.7 4.2 1.3]\t\t1\t\t[1]\n",
      "[6.3 2.3 4.4 1.3]\t\t1\t\t[1]\n",
      "[4.7 3.2 1.6 0.2]\t\t0\t\t[0]\n",
      "[4.7 3.2 1.3 0.2]\t\t0\t\t[0]\n",
      "[6.1 3.  4.9 1.8]\t\t2\t\t[2]\n",
      "[5.1 3.8 1.9 0.4]\t\t0\t\t[0]\n",
      "[7.2 3.2 6.  1.8]\t\t2\t\t[2]\n"
     ]
    }
   ],
   "source": [
    "print(\"SL|SW|PL|PW \\t\\t Predicted Class \\t\\t Actual Class\")\n",
    "for i in range(len(xtest)):\n",
    "    predict = nnet.feedforward(xtest[i])\n",
    "    predicted_class = np.argmax(predict)\n",
    "    actual = ytest[i]\n",
    "    print(f\"{xtest[i]}\\t\\t{predicted_class}\\t\\t{actual}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5ff2e695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.67%\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "total = len(xtest)\n",
    "for i in range(total):\n",
    "    \n",
    "    predict = nnet.feedforward(xtest[i])\n",
    "    predicted_class = np.argmax(predict)\n",
    "    actual = ytest[i]\n",
    "    if predicted_class == actual:\n",
    "        count+=1\n",
    "accuracy = (count/total)*100\n",
    "print(f\"{accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b371c5e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.66666666666667"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred=nnet.feedforward(xtest)\n",
    "ypred_classes=[np.argmax(element) for element in ypred]\n",
    "ypred_classes\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc=accuracy_score(ytest,ypred_classes)\n",
    "acc*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "004b7655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2766801422847673\n",
      "Loss: 0.11103768300876314\n",
      "Loss: 0.036414081298737265\n",
      "Loss: 0.01697178819142473\n",
      "Loss: 0.01023176478619698\n",
      "Loss: 0.007094993054699653\n",
      "Loss: 0.0053450908317975495\n",
      "Loss: 0.00424888709142822\n",
      "Loss: 0.003505623173197557\n",
      "Loss: 0.002972087015387712\n",
      "Loss: 0.0025723251057786513\n",
      "Loss: 0.002262651416714106\n",
      "Loss: 0.0020163027592903834\n",
      "Loss: 0.0018160357284023989\n",
      "Loss: 0.0016502737257416337\n",
      "Loss: 0.0015109741313561248\n",
      "Loss: 0.0013923857822582867\n",
      "Loss: 0.0012902932959198952\n",
      "Loss: 0.0012015402937883867\n",
      "Loss: 0.001123718900806483\n",
      "Loss: 0.00105496186353036\n",
      "Loss: 0.0009937999456352848\n",
      "Loss: 0.0009390619710587561\n",
      "Loss: 0.0008898033993412393\n",
      "Loss: 0.0008452543993708167\n",
      "Loss: 0.0008047815051525032\n",
      "Loss: 0.000767858897473425\n",
      "Loss: 0.000734046615774795\n",
      "Loss: 0.0007029738316003046\n",
      "Loss: 0.0006743258678074592\n",
      "Loss: 0.0006478340235499332\n",
      "Loss: 0.0006232675245439418\n",
      "Loss: 0.0006004270999121642\n",
      "Loss: 0.0005791398159380828\n",
      "Loss: 0.0005592548898014059\n",
      "Loss: 0.00054064027378307\n",
      "Loss: 0.0005231798499675302\n",
      "Loss: 0.0005067711122377575\n",
      "Loss: 0.0004913232399052144\n",
      "Loss: 0.0004767554881375338\n",
      "Loss: 0.00046299583621489345\n",
      "Loss: 0.0004499798468342877\n",
      "Loss: 0.00043764969911260306\n",
      "Loss: 0.0004259533652883898\n",
      "Loss: 0.00041484390688692203\n",
      "Loss: 0.00040427887066337673\n",
      "Loss: 0.0003942197682519175\n",
      "Loss: 0.0003846316263336563\n",
      "Loss: 0.000375482596452955\n",
      "Loss: 0.00036674361548081605\n",
      "Loss: 0.00035838810924022294\n",
      "Loss: 0.0003503917330434092\n",
      "Loss: 0.0003427321439021577\n",
      "Loss: 0.0003353888000030688\n",
      "Loss: 0.00032834278372572377\n",
      "Loss: 0.0003215766450498953\n",
      "Loss: 0.0003150742626707101\n",
      "Loss: 0.00030882072053525886\n",
      "Loss: 0.0003028021978447153\n",
      "Loss: 0.00029700587084387095\n",
      "Loss: 0.000291419824954312\n",
      "Loss: 0.0002860329760056188\n",
      "Loss: 0.0002808349994872026\n",
      "Loss: 0.00027581626688638497\n",
      "Loss: 0.000270967788300475\n",
      "Loss: 0.0002662811606150842\n",
      "Loss: 0.0002617485206304585\n",
      "Loss: 0.00025736250259477387\n",
      "Loss: 0.00025311619966969994\n",
      "Loss: 0.0002490031289110242\n",
      "Loss: 0.0002450171993968039\n",
      "Loss: 0.00024115268317876552\n",
      "Loss: 0.00023740418877015685\n",
      "Loss: 0.0002337666369160069\n",
      "Loss: 0.00023023523842046628\n",
      "Loss: 0.00022680547383073318\n",
      "Loss: 0.00022347307479924744\n",
      "Loss: 0.00022023400696496344\n",
      "Loss: 0.0002170844542115546\n",
      "Loss: 0.00021402080417541028\n",
      "Loss: 0.00021103963488940992\n",
      "Loss: 0.00020813770246027257\n",
      "Loss: 0.00020531192968755644\n",
      "Loss: 0.0002025593955416341\n",
      "Loss: 0.00019987732542613504\n",
      "Loss: 0.00019726308215762493\n",
      "Loss: 0.00019471415760177638\n",
      "Loss: 0.00019222816491111877\n",
      "Loss: 0.0001898028313145817\n",
      "Loss: 0.00018743599141373096\n",
      "Loss: 0.0001851255809447298\n",
      "Loss: 0.00018286963096880726\n",
      "Loss: 0.00018066626245736287\n",
      "Loss: 0.00017851368124085712\n",
      "Loss: 0.0001764101732933534\n",
      "Loss: 0.00017435410032704685\n",
      "Loss: 0.00017234389567331103\n",
      "Loss: 0.00017037806042881993\n",
      "Loss: 0.00016845515984707132\n",
      "Loss: 0.00016657381995736253\n",
      "Input:  [[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "Actual Output:  [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "Loss:  0.00016473272439466096\n",
      "\n",
      "\n",
      "Predicted Output:  [[0.01517596]\n",
      " [0.98804001]\n",
      " [0.98771916]\n",
      " [0.01160865]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# X = (hours sleeping, hours studying), y = test score of the student\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# scale units\n",
    "# X = X/np.amax(X, axis=0) #maximum of X array\n",
    "# y = y/100 # maximum test score is 100\n",
    "# X=np.array([[0,0,1,1,0,0,0,0],\n",
    "#                [0,0,1,1,0,0,0,1],\n",
    "#                [0,0,1,1,0,0,1,0],\n",
    "#                [0,0,1,1,0,0,1,1],\n",
    "#                [0,0,1,1,0,1,0,0],\n",
    "#                [0,0,1,1,0,1,0,1],\n",
    "#                [0,0,1,1,0,1,1,0],\n",
    "#                [0,0,1,1,0,1,1,1],\n",
    "#                [0,0,1,1,1,0,0,0],\n",
    "#                [0,0,1,1,1,0,0,1]])\n",
    "\n",
    "# y=np.array([[0],[1],[0],[1],[0],[1],[0],[1],[0],[1]])\n",
    "class NeuralNetwork(object):\n",
    "    def __init__(self):\n",
    "        #parameters\n",
    "        self.inputSize = 2\n",
    "        self.outputSize = 1\n",
    "        self.hiddenSize = 15\n",
    "        self.bias1=0\n",
    "        self.bias2=0\n",
    "        #weights\n",
    "        self.W1 = np.random.randn(self.inputSize, self.hiddenSize) # (3x2) weight matrix from input to hidden layer\n",
    "        self.W2 = np.random.randn(self.hiddenSize, self.outputSize) # (3x1) weight matrix from hidden to output layer\n",
    "        \n",
    "    def feedForward(self, X):\n",
    "        #forward propogation through the network\n",
    "        self.z = np.dot(X, self.W1)+self.bias1 #dot product of X (input) and first set of weights (3x2)\n",
    "        self.z2 = self.sigmoid(self.z) #activation function\n",
    "        self.z3 = np.dot(self.z2, self.W2)+self.bias2 #dot product of hidden layer (z2) and second set of weights (3x1)\n",
    "        output = self.sigmoid(self.z3)\n",
    "        return output\n",
    "        \n",
    "    def sigmoid(self, s, deriv=False):\n",
    "        if (deriv == True):\n",
    "            return s * (1 - s)\n",
    "        return 1/(1 + np.exp(-s))\n",
    "    \n",
    "    def backward(self, X, y, output,lr):\n",
    "        #backward propogate through the network\n",
    "        self.output_error = y - output # error in output\n",
    "        self.output_delta = self.output_error * self.sigmoid(output, deriv=True)\n",
    "        \n",
    "        self.z2_error = self.output_delta.dot(self.W2.T) #z2 error: how much our hidden layer weights contribute to output error\n",
    "        self.z2_delta = self.z2_error * self.sigmoid(self.z2, deriv=True) #applying derivative of sigmoid to z2 error\n",
    "        \n",
    "        self.W1 += X.T.dot(self.z2_delta) # adjusting first set (input -> hidden) weights\n",
    "        self.W2 += self.z2.T.dot(self.output_delta) # adjusting second set (hidden -> output) weights\n",
    "        \n",
    "        self.bias1+=np.sum(self.z2_delta)*lr\n",
    "        self.bias2+=np.sum(self.output_delta)*lr\n",
    "        \n",
    "        \n",
    "    def train(self, X, y):\n",
    "        output = self.feedForward(X)\n",
    "        self.backward(X, y, output,0.1)\n",
    "        \n",
    "NN = NeuralNetwork()\n",
    "\n",
    "for i in range(10000): #trains the NN 1000 times\n",
    "    if (i % 100 == 0):\n",
    "        print(\"Loss: \" + str(np.mean(np.square(y - NN.feedForward(X)))))\n",
    "        pass\n",
    "    NN.train(X, y)\n",
    "        \n",
    "print(\"Input: \" , (X))\n",
    "print(\"Actual Output: \" , (y))\n",
    "print(\"Loss: \" , (np.mean(np.square(y - NN.feedForward(X)))))\n",
    "print(\"\\n\")\n",
    "print(\"Predicted Output: \" ,(NN.feedForward(X)))\n",
    "\n",
    "ypred=NN.feedForward(X)\n",
    "\n",
    "predict=NN.feedForward(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3327cd16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ba0394d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1) if predict<0.5 else print(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1061d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.input_size = 4\n",
    "        self.output_size = 3\n",
    "        self.hidden1_size = 50\n",
    "        self.hidden2_size = 50\n",
    "        self.bias1 = np.random.randn(1,(self.hidden1_size))\n",
    "        self.bias2 = np.random.randn(1,(self.hidden2_size))\n",
    "        self.bias3 = np.random.randn(1,(self.output_size))\n",
    "        \n",
    "        self.W1 = np.random.randn(self.input_size, self.hidden1_size)\n",
    "        self.W2 = np.random.randn(self.hidden1_size, self.hidden2_size)\n",
    "        self.W3 = np.random.randn(self.hidden2_size, self.output_size)\n",
    "    \n",
    "    def feedForward(self, X):\n",
    "        self.input_sum1 = np.dot(X, self.W1) + self.bias1\n",
    "        self.hid_inp1 = self.sigmoid(self.input_sum1)\n",
    "        \n",
    "        self.input_sum2 = np.dot(self.hid_inp1, self.W2) + self.bias2\n",
    "        self.hid_inp2 = self.sigmoid(self.input_sum2)\n",
    "        \n",
    "        self.input_sum3 = np.dot(self.hid_inp2, self.W3) + self.bias3\n",
    "        output = self.sigmoid(self.input_sum3)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backpropagation(self, X, y, output, lr):\n",
    "        self.error = y - output\n",
    "        self.output_delta = self.error * self.sigmoid_derivative(output)\n",
    "        \n",
    "        self.hidden2_error = self.output_delta.dot(self.W3.T)\n",
    "        self.hidden2_delta = self.hidden2_error * self.sigmoid_derivative(self.hid_inp2)\n",
    "        \n",
    "        self.hidden1_error = self.hidden2_delta.dot(self.W2.T)\n",
    "        self.hidden1_delta = self.hidden1_error * self.sigmoid_derivative(self.hid_inp1)\n",
    "\n",
    "        self.W1 += X.T.dot(self.hidden1_delta) * lr\n",
    "        self.W2 += self.hid_inp1.T.dot(self.hidden2_delta) * lr\n",
    "        self.W3 += self.hid_inp2.T.dot(self.output_delta) * lr\n",
    "\n",
    "        self.bias1 += np.sum(self.hidden1_delta) * lr\n",
    "        self.bias2 += np.sum(self.hidden2_delta) * lr\n",
    "        self.bias3 += np.sum(self.output_delta) * lr\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, s):\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        output = self.feedForward(X)\n",
    "        self.backpropagation(X, y, output, 0.1)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "824b41fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
       "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
       "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
       "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
       "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
       "4   5            5.0           3.6            1.4           0.2  Iris-setosa"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('Iris.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9ff6c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "LE=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7649cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm',\n",
       "       'Species'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Species']=LE.fit_transform(df['Species'])\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e2895726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "oe=OneHotEncoder()\n",
    "x=np.array([df['SepalLengthCm'],df['SepalWidthCm'],df['PetalLengthCm'],df['PetalWidthCm']]).T\n",
    "y=np.array([df['Species']]).T\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3198d006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_hot = oe.fit_transform(y.reshape(-1, 1)).toarray()\n",
    "y_train_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2eafe1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "NN=NeuralNetwork()\n",
    "for i in range(10000):\n",
    "    NN.train(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "129e5548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.6666666662626493\n",
      "\n",
      "\n",
      "Predicted Output:  [[1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         0.99999999 1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# print(\"Input: \" , (x))\n",
    "# print(\"Actual Output: \" , (y))\n",
    "print(\"Loss: \" , (np.mean(np.square(y - NN.feedForward(x)))))\n",
    "print(\"\\n\")\n",
    "print(\"Predicted Output: \" ,(NN.feedForward(x)))\n",
    "\n",
    "ypred=NN.feedForward(x)\n",
    "\n",
    "# predict=NN.feedForward(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b5ecdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f179c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3deb9800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss= 0.38388625491969613\n",
      "Loss= 0.2764609440588489\n",
      "Loss= 0.25585836682037866\n",
      "Loss= 0.2449608670268129\n",
      "Loss= 0.24028192369941082\n",
      "Loss= 0.23594449501646253\n",
      "Loss= 0.2325842770257183\n",
      "Loss= 0.2299573366982345\n",
      "Loss= 0.22760564767535169\n",
      "Loss= 0.22584719508980233\n",
      "Loss= 0.2251643520994801\n",
      "Loss= 0.22311823043262347\n",
      "Loss= 0.22197307831975033\n",
      "Loss= 0.22105995182398658\n",
      "Loss= 0.22036815016207612\n",
      "Loss= 0.2205828484860132\n",
      "Loss= 0.24476910341962915\n",
      "Loss= 0.21870440718227047\n",
      "Loss= 0.24445454797795252\n",
      "Loss= 0.2181475021490068\n",
      "Result=\n",
      " [[9.93936926e-01 6.06307406e-03 1.49312829e-19]\n",
      " [3.33333333e-01 3.33333333e-01 3.33333333e-01]\n",
      " [3.33333333e-01 3.33333333e-01 3.33333333e-01]]\n",
      "Actual:\n",
      " [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "Loss= 0.24481178960710448\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# X = (hours sleeping, hours studying), y = test score of the student\n",
    "X = np.array(([2, 9], [1, 5], [3, 6]))\n",
    "y = np.array(([1, 0, 0], [0, 1, 0], [0, 0, 1]))  # One-hot encoding for multiclass classification\n",
    "\n",
    "# scale units\n",
    "X = X / np.amax(X, axis=0)  # maximum of X array\n",
    "# No need to scale y since it's already one-hot encoded\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.inp_size = 2\n",
    "        self.output_size = 3  # Number of classes\n",
    "        self.hidden1_size = 3\n",
    "        self.hidden2_size = 5\n",
    "        self.bias1 = 0\n",
    "        self.bias2 = 0\n",
    "        self.bias3 = 0\n",
    "\n",
    "        self.W1 = np.random.randn(self.inp_size, self.hidden1_size)\n",
    "        self.W2 = np.random.randn(self.hidden1_size, self.hidden2_size)\n",
    "        self.W3 = np.random.randn(self.hidden2_size, self.output_size)\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_values = np.exp(x)\n",
    "        return exp_values / np.sum(exp_values,axis=1,keepdims=True)\n",
    "\n",
    "    def feedforward(self, X):\n",
    "        self.input_sum1 = np.dot(X, self.W1) + self.bias1\n",
    "        self.hid1_inp = self.relu(self.input_sum1)\n",
    "\n",
    "        self.input_sum2 = np.dot(self.hid1_inp, self.W2) + self.bias2\n",
    "        self.hid2_inp = self.relu(self.input_sum2)\n",
    "\n",
    "        self.input_sum3 = np.dot(self.hid2_inp, self.W3) + self.bias3\n",
    "        output = self.softmax(self.input_sum3)\n",
    "        return output\n",
    "\n",
    "    def relu_derivative(self, x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "\n",
    "    def backpropagation(self, X, y, output, lr):\n",
    "        self.error = y - output\n",
    "        self.output_delta = self.error  # No need to multiply by derivative of softmax as it's included in the derivative of cross-entropy loss\n",
    "\n",
    "        self.hidden2_error = self.output_delta.dot(self.W3.T)\n",
    "        self.hidden2_delta = self.hidden2_error * self.relu_derivative(self.hid2_inp)\n",
    "\n",
    "        self.hidden1_error = self.hidden2_delta.dot(self.W2.T)\n",
    "        self.hidden1_delta = self.hidden1_error * self.relu_derivative(self.hid1_inp)\n",
    "\n",
    "        self.W1 += X.T.dot(self.hidden1_delta) * lr\n",
    "        self.W2 += self.hid1_inp.T.dot(self.hidden2_delta) * lr\n",
    "        self.W3 += self.hid2_inp.T.dot(self.output_delta) * lr\n",
    "\n",
    "        self.bias1 += np.sum(self.hidden1_delta) * lr\n",
    "        self.bias2 += np.sum(self.hidden2_delta) * lr\n",
    "        self.bias3 += np.sum(self.output_delta) * lr\n",
    "\n",
    "    def train(self, X, y):\n",
    "        output = self.feedforward(X)\n",
    "        self.backpropagation(X, y, output, 0.01)\n",
    "\n",
    "\n",
    "NN = NeuralNetwork()\n",
    "\n",
    "for i in range(10000):\n",
    "    if i % 500 == 0:\n",
    "        print('Loss=', -np.mean(y * np.log(NN.feedforward(X))))\n",
    "    NN.train(X, y)\n",
    "\n",
    "print('Result=\\n', NN.feedforward(X))\n",
    "print('Actual:\\n', y)\n",
    "print('Loss=', -np.mean(y * np.log(NN.feedforward(X))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9cd172",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
