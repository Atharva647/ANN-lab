{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a5a30a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc7df57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralnet():\n",
    "    def __init__(self,inp_size,hidd_size,output_size,lr):\n",
    "        self.input_size=inp_size\n",
    "        self.hidden_size=hidd_size\n",
    "        self.output_size=output_size\n",
    "        self.lr=lr\n",
    "        self.bias1=0\n",
    "        self.bias2=0\n",
    "        \n",
    "        self.w1=np.random.uniform(-0.5,-0.5,(self.input_size,self.hidden_size))\n",
    "        self.w2=np.random.uniform(-0.5,-0.5,(self.hidden_size,self.output_size))\n",
    "        \n",
    "    def relu(self,x):\n",
    "        return np.maximum(0,x)\n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "\n",
    "    \n",
    "    def sigmoid(self,x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self,s):\n",
    "        return s*(1-s)\n",
    "    \n",
    "    def forwardpass(self,x):\n",
    "        self.summation=np.dot(x,self.w1)+self.bias1\n",
    "        self.hid_inp=self.relu(self.summation)\n",
    "        \n",
    "        self.hid_sum=np.dot(self.hid_inp,self.w2)\n",
    "        output=self.sigmoid(self.hid_sum)\n",
    "        return output\n",
    "    \n",
    "    def backpropagation(self,x,y,output):\n",
    "        self.error=y-output\n",
    "        self.output_delta=self.error*self.sigmoid_derivative(output)\n",
    "        \n",
    "        self.hidd_error=self.output_delta.dot(self.w2.T)\n",
    "        self.hidd_delta=self.hidd_error*self.relu_derivative(self.hid_inp)\n",
    "        \n",
    "        self.w1+=x.T.dot(self.hidd_delta)*self.lr\n",
    "        self.w2+=self.hid_inp.T.dot(self.output_delta)*self.lr\n",
    "        \n",
    "#         self.bias1+=np.sum(self.hidd_delta)*self.lr\n",
    "#         self.bias2+=np.sum(self.output_delta)*self.lr\n",
    "#         self.error = y - output\n",
    "#         self.output_delta = self.error * self.relu_derivative(output)\n",
    "# #         print(\"Shape of self.output_delta:\", self.output_delta.shape)\n",
    "#         # Transpose w2 to match the shape of output_delta for dot product\n",
    "#         self.hidd_error = self.output_delta.dot(self.w2.T)\n",
    "#         self.hidd_delta = self.hidd_error * self.relu_derivative(self.hid_inp)\n",
    "\n",
    "        # Update weights\n",
    "        self.w1 += x.T.dot(self.hidd_delta) * self.lr\n",
    "        self.w2 += self.hid_inp.T.dot(self.output_delta) * self.lr\n",
    "        \n",
    "    def train(self,x,y,epochs):\n",
    "        for epoch in range(epochs):\n",
    "            output=self.forwardpass(x)\n",
    "            self.backpropagation(x,y,output)\n",
    "            if epoch%250==0:\n",
    "                print('loss: ',np.mean(np.square(y-output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "973668ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x=np.array([[0,0],[1,1],[1,0],[0,1]])\n",
    "# y=np.array([[0],[0],[1],[1]])\n",
    "# inp_size=x.shape[1]\n",
    "# op_size=y.shape[1]\n",
    "# hidden_size=100\n",
    "# NN=neuralnet(inp_size,hidden_size,op_size,0.1)\n",
    "# x.shape[1],y.shape[1]\n",
    "\n",
    "# # NN.train(x,y,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "17a555e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN.train(x,y,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "61324139",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (150,4) and (2,100) not aligned: 4 (dim 1) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(NN\u001b[38;5;241m.\u001b[39mforwardpass(x))\n",
      "Cell \u001b[1;32mIn[10], line 27\u001b[0m, in \u001b[0;36mneuralnet.forwardpass\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforwardpass\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msummation\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mdot(x,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw1)\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias1\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhid_inp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msummation)\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhid_sum\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhid_inp,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw2)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (150,4) and (2,100) not aligned: 4 (dim 1) != 2 (dim 0)"
     ]
    }
   ],
   "source": [
    "print(NN.forwardpass(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cc6857e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Atharva\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "iris = datasets.load_iris()\n",
    "x = iris.data\n",
    "y = iris.target.reshape(-1,1)\n",
    "\n",
    "encoder = OneHotEncoder(sparse = False)\n",
    "y = encoder.fit_transform(y)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=0.2,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2a015d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=0.2,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e54e019e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 4\n",
    "hidden_size = 100\n",
    "output_size = 3\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2c7dfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Initialize weights and biases for hidden layer\n",
    "        self.weights_hidden = np.random.randn(self.input_size, self.hidden_size)\n",
    "        self.bias_hidden = np.zeros((1, self.hidden_size))\n",
    "        \n",
    "        # Initialize weights and biases for output layer\n",
    "        self.weights_output = np.random.randn(self.hidden_size, self.output_size)\n",
    "        self.bias_output = np.zeros((1, self.output_size))\n",
    "        \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_values = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "    \n",
    "    def forward_pass(self, x):\n",
    "        # Forward pass through hidden layer\n",
    "        hidden_layer_input = np.dot(x, self.weights_hidden) + self.bias_hidden\n",
    "        self.hidden_layer_output = self.relu(hidden_layer_input)\n",
    "        \n",
    "        # Forward pass through output layer\n",
    "        output_layer_input = np.dot(self.hidden_layer_output, self.weights_output) + self.bias_output\n",
    "        output = self.softmax(output_layer_input)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward_pass(self, x, y, output):\n",
    "        error = output - y\n",
    "        \n",
    "        # Backpropagation through output layer\n",
    "        delta_output = error * (output * (1 - output))\n",
    "        grad_weights_output = np.dot(self.hidden_layer_output.T, delta_output)\n",
    "        grad_bias_output = np.sum(delta_output, axis=0, keepdims=True)\n",
    "        \n",
    "        # Backpropagation through hidden layer\n",
    "        delta_hidden = np.dot(delta_output, self.weights_output.T) * (self.hidden_layer_output > 0)\n",
    "        grad_weights_hidden = np.dot(x.T, delta_hidden)\n",
    "        grad_bias_hidden = np.sum(delta_hidden, axis=0, keepdims=True)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.weights_hidden -= self.learning_rate * grad_weights_hidden\n",
    "        self.bias_hidden -= self.learning_rate * grad_bias_hidden\n",
    "        self.weights_output -= self.learning_rate * grad_weights_output\n",
    "        self.bias_output -= self.learning_rate * grad_bias_output\n",
    "    \n",
    "    def train(self, x, y, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            output = self.forward_pass(x)\n",
    "            self.backward_pass(x, y, output)\n",
    "            if epoch % 100 == 0:\n",
    "                loss = -np.mean(y * np.log(output))\n",
    "                print(f\"Epoch {epoch}: Loss {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "575caef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NeuralNetwork(input_size, hidden_size, output_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd5e22a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 35.89013721421776\n",
      "Epoch 100: Loss 35.89013721421776\n",
      "Epoch 200: Loss 35.89013721421776\n",
      "Epoch 300: Loss 35.89013721421776\n",
      "Epoch 400: Loss 35.89013721421776\n",
      "Epoch 500: Loss 35.89013721421776\n",
      "Epoch 600: Loss 35.89013721421776\n",
      "Epoch 700: Loss 35.89013721421776\n",
      "Epoch 800: Loss 35.89013721421776\n",
      "Epoch 900: Loss 35.89013721421776\n",
      "Epoch 1000: Loss 35.89013721421776\n",
      "Epoch 1100: Loss 35.89013721421776\n",
      "Epoch 1200: Loss 35.89013721421776\n",
      "Epoch 1300: Loss 35.89013721421776\n",
      "Epoch 1400: Loss 35.89013721421776\n",
      "Epoch 1500: Loss 35.89013721421776\n",
      "Epoch 1600: Loss 35.89013721421776\n",
      "Epoch 1700: Loss 35.89013721421776\n",
      "Epoch 1800: Loss 35.89013721421776\n",
      "Epoch 1900: Loss 35.89013721421776\n",
      "Epoch 2000: Loss 35.89013721421776\n",
      "Epoch 2100: Loss 35.89013721421776\n",
      "Epoch 2200: Loss 35.89013721421776\n",
      "Epoch 2300: Loss 35.89013721421776\n",
      "Epoch 2400: Loss 35.89013721421776\n",
      "Epoch 2500: Loss 35.89013721421776\n",
      "Epoch 2600: Loss 35.89013721421776\n",
      "Epoch 2700: Loss 35.89013721421776\n",
      "Epoch 2800: Loss 35.89013721421776\n",
      "Epoch 2900: Loss 35.89013721421776\n",
      "Epoch 3000: Loss 35.89013721421776\n",
      "Epoch 3100: Loss 35.89013721421776\n",
      "Epoch 3200: Loss 35.89013721421776\n",
      "Epoch 3300: Loss 35.89013721421776\n",
      "Epoch 3400: Loss 35.89013721421776\n",
      "Epoch 3500: Loss 35.89013721421776\n",
      "Epoch 3600: Loss 35.89013721421776\n",
      "Epoch 3700: Loss 35.89013721421776\n",
      "Epoch 3800: Loss 35.89013721421776\n",
      "Epoch 3900: Loss 35.89013721421776\n",
      "Epoch 4000: Loss 35.89013721421776\n",
      "Epoch 4100: Loss 35.89013721421776\n",
      "Epoch 4200: Loss 35.89013721421776\n",
      "Epoch 4300: Loss 35.89013721421776\n",
      "Epoch 4400: Loss 35.89013721421776\n",
      "Epoch 4500: Loss 35.89013721421776\n",
      "Epoch 4600: Loss 35.89013721421776\n",
      "Epoch 4700: Loss 35.89013721421776\n",
      "Epoch 4800: Loss 35.89013721421776\n",
      "Epoch 4900: Loss 35.89013721421776\n",
      "Epoch 5000: Loss 35.89013721421776\n",
      "Epoch 5100: Loss 35.89013721421776\n",
      "Epoch 5200: Loss 35.89013721421776\n",
      "Epoch 5300: Loss 35.89013721421776\n",
      "Epoch 5400: Loss 35.89013721421776\n",
      "Epoch 5500: Loss 35.89013721421776\n",
      "Epoch 5600: Loss 35.89013721421776\n",
      "Epoch 5700: Loss 35.89013721421776\n",
      "Epoch 5800: Loss 35.89013721421776\n",
      "Epoch 5900: Loss 35.89013721421776\n",
      "Epoch 6000: Loss 35.89013721421776\n",
      "Epoch 6100: Loss 35.89013721421776\n",
      "Epoch 6200: Loss 35.89013721421776\n",
      "Epoch 6300: Loss 35.89013721421776\n",
      "Epoch 6400: Loss 35.89013721421776\n",
      "Epoch 6500: Loss 35.89013721421776\n",
      "Epoch 6600: Loss 35.89013721421776\n",
      "Epoch 6700: Loss 35.89013721421776\n",
      "Epoch 6800: Loss 35.89013721421776\n",
      "Epoch 6900: Loss 35.89013721421776\n",
      "Epoch 7000: Loss 35.89013721421776\n",
      "Epoch 7100: Loss 35.89013721421776\n",
      "Epoch 7200: Loss 35.89013721421776\n",
      "Epoch 7300: Loss 35.89013721421776\n",
      "Epoch 7400: Loss 35.89013721421776\n",
      "Epoch 7500: Loss 35.89013721421776\n",
      "Epoch 7600: Loss 35.89013721421776\n",
      "Epoch 7700: Loss 35.89013721421776\n",
      "Epoch 7800: Loss 35.89013721421776\n",
      "Epoch 7900: Loss 35.89013721421776\n",
      "Epoch 8000: Loss 35.89013721421776\n",
      "Epoch 8100: Loss 35.89013721421776\n",
      "Epoch 8200: Loss 35.89013721421776\n",
      "Epoch 8300: Loss 35.89013721421776\n",
      "Epoch 8400: Loss 35.89013721421776\n",
      "Epoch 8500: Loss 35.89013721421776\n",
      "Epoch 8600: Loss 35.89013721421776\n",
      "Epoch 8700: Loss 35.89013721421776\n",
      "Epoch 8800: Loss 35.89013721421776\n",
      "Epoch 8900: Loss 35.89013721421776\n",
      "Epoch 9000: Loss 35.89013721421776\n",
      "Epoch 9100: Loss 35.89013721421776\n",
      "Epoch 9200: Loss 35.89013721421776\n",
      "Epoch 9300: Loss 35.89013721421776\n",
      "Epoch 9400: Loss 35.89013721421776\n",
      "Epoch 9500: Loss 35.89013721421776\n",
      "Epoch 9600: Loss 35.89013721421776\n",
      "Epoch 9700: Loss 35.89013721421776\n",
      "Epoch 9800: Loss 35.89013721421776\n",
      "Epoch 9900: Loss 35.89013721421776\n"
     ]
    }
   ],
   "source": [
    "network.train(x_train,y_train,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4a8d6bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26666666666666666"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=network.forward_pass(x_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "prediction = np.argmax(pred, axis = 1)\n",
    "accuracy = np.mean(prediction == np.argmax(y_test, axis = 1))\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8639dde0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
